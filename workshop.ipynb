{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de55d46a",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysing Zoo Reviews with Natural Language Processing (NLP)\n",
    "---\n",
    "\n",
    "#### Getting Started with the Notebook\n",
    "\n",
    "Most of the code in this notebook is already written for you. Your job is to run the cells and explore the outputs to understand what's happening. You're free to explore and change parameters and see how the results change!\n",
    "\n",
    "\n",
    "To run any code cell:\n",
    "\n",
    "- Click inside the cell\n",
    "\n",
    "- Press `Shift + Enter` (or use the ▶ button at the top of the notebook)\n",
    "\n",
    "Make sure to run the cells **in order**, from top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef25fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run this cell to print this text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc5416",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "Imagine you work at **Edinburgh Zoo** and your job is to help improve the experience for future visitors. Every day, people write online reviews describing what they liked (or didn't like) about their visit. These reviews contain valuable feedback - but they are written in free-form text and can quickly become overwhelming.\n",
    "\n",
    "If you had only ten reviews, you might be able to read them all yourself and spot patterns. But what if you had hundreds or thousands?\n",
    "\n",
    "That's where **Natural Language Processing (NLP)** comes in. NLP is a field of Artificial Intelligence that allows us to extract structure and meaning from human language. By combining a set of smart tools, we can automatically process visitor reviews, identify which animals are mentioned, understand the emotions expressed about them, and summarise the main opinions - all without having to read every sentence ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343547a",
   "metadata": {},
   "source": [
    "#### What We're Trying to Do\n",
    "\n",
    "We want to answer questions such as:\n",
    "\n",
    "- *Which animals do visitors **mention most**?*\n",
    "\n",
    "- *What do people **like** or **dislike** about different animals?*\n",
    "\n",
    "- *Are there any consistent **complaints** or **highlights**?*\n",
    "\n",
    "- *Can we produce a **summary for each animal** based on real visitor feedback?*\n",
    "\n",
    "To do this, we will go step by step from raw reviews to structured summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f709dd1",
   "metadata": {},
   "source": [
    "#### Why Not Just Ask ChatGPT?\n",
    "\n",
    "You might wonder: *why not simply copy all the reviews into a tool like ChatGPT and ask it to summarise them?*\n",
    "\n",
    "There are two key reasons:\n",
    "\n",
    "1. **Scale and Structure:** LLMs work best when given well-structured, focused inputs. Feeding in an entire review dataset - thousands of unrelated sentences - without any filtering or organisation would produce vague or unreliable outputs.\n",
    "\n",
    "2. **Transparency and Control:** By splitting the task into parts (e.g. identifying animals, measuring sentiment, visualising results), we can understand what is happening at each stage. This not only builds trust in the results but also gives us flexibility to tune or troubleshoot specific steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bc2697",
   "metadata": {},
   "source": [
    "In this notebook, we will build a simple pipeline using Python and some powerful pre-trained models to:\n",
    "\n",
    "1. **Clean** and **prepare** the review data\n",
    "\n",
    "2. **Find animals** in sentences\n",
    "\n",
    "3. Identify **to what animal in the zoo** the sentence refers to \n",
    "\n",
    "4. Run **sentiment analysis** to detect the emotion of the sentence\n",
    "\n",
    "5. Create plots to **visualise** sentiment per animal\n",
    "\n",
    "6. Generate short, readable **summaries** for each animal using a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e4633",
   "metadata": {},
   "source": [
    "#### Tools We'll Use\n",
    "\n",
    "We'll be using the following tools and libraries:\n",
    "\n",
    "| Tool / Library                          | Purpose                                                                 |\n",
    "|----------------------------------------|-------------------------------------------------------------------------|\n",
    "| `pandas`                               | For organising and analysing tabular data                              |\n",
    "| `transformers` (Hugging Face)          | For using pre-trained models (NER, sentiment analysis, and LLMs)       |\n",
    "| `torch`                                | Works alongside `transformers` to train and run powerful ML models     |\n",
    "| `spacy`                                | For splitting paragraphs into individual sentences                     |\n",
    "| `matplotlib`                           | For plotting visualisations                                            |\n",
    "| `re` (regular expressions)             | Temporary method for detecting animal mentions in text (to be replaced by NER) |\n",
    "| `tqdm`                                 | Adds a progress bar for processes that take longer                     |\n",
    "| `rapidfuzz`                            | Compares strings to fix typos and variations                           |\n",
    "| Pretrained model (BERT)                | Used to detect animal names in sentences                               |\n",
    "| Local large language model (Mistral-7B) | Generates natural-language summaries from grouped reviews              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575688df",
   "metadata": {},
   "source": [
    "--- \n",
    "## Step 0. Set up the environment\n",
    "---\n",
    "This notebook has been prepared in python version 3.12.2. \n",
    "\n",
    "To create the environment, run the following commands in a terminal:\n",
    "```bash\n",
    "# Create a python environment (you can create a Conda environment instead)\n",
    "python -m venv sutton_venv\n",
    "\n",
    "# Activate environment\n",
    "source sutton_venv/bin/activate\n",
    "\n",
    "# Install packages\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Allow jupyter notebook to access the environment\n",
    "python -m ipykernel install --user\n",
    "\n",
    "# Install needed spacy model\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "Alternatively, if `pip install -r requirements.txt` line runs into errors, install the needed packages yourself through running:\n",
    "```bash\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user\n",
    "pip install pandas matplotlib transformers datasets spacy seqeval torch rapidfuzz \n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "\n",
    "To activate the environment, in VSCode:\n",
    "* Select kernel -> download the recommended extensions (if prompted)\n",
    "* Select kernel -> Python Environment... > NER_venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6999ba",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load and Explore the Review Data\n",
    "---\n",
    "\n",
    "We'll be using real visitor reviews collected from an online platform (TripAdvisor), where people share their experiences at Edinburgh Zoo.\n",
    "\n",
    "To save time, the data has already been collected and lightly cleaned. It's stored in a CSV file called `edinburgh-zoo-reviews-1990.csv`, with 1990 reviews. Each row contains:\n",
    "- A **title** (short summary),\n",
    "- And a **full review text** (the visitor's main comments).\n",
    "\n",
    "Run the following code cell to load the data and take a look at a few example reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# Load the review data from the CSV file\n",
    "df = pd.read_csv(\"edinburgh-zoo-reviews-1990.csv\")\n",
    "\n",
    "# Change the maximum column width to display the full review\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fcedd7",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  border-left: 5px solid #4caf50;\n",
    "  background: #f6ffed;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong>Try this:</strong> Change the number inside <code>head()</code> to see more reviews!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc221403",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Prepare the Review Text\n",
    "---\n",
    "\n",
    "Before we can analyse the reviews, we need to **prepare the text**. Online reviews - like those from TripAdvisor - are often informal. That means they might contain:\n",
    "\n",
    "- Long paragraphs with multiple ideas  \n",
    "- Inconsistent punctuation or formatting  \n",
    "- Occasional typos or grammar mistakes  \n",
    "\n",
    "All of this can make it harder for our analysis to be accurate or meaningful. So before we go further, we'll do one key thing:\n",
    "\n",
    "- **Split each review into individual sentences** - to break down complex reviews into smaller, more focused pieces of text.\n",
    "\n",
    "<!-- 💡 Why Split Reviews Into Sentences? -->\n",
    "<div style=\"\n",
    "  border-left: 5px solid #ffc107;\n",
    "  background: #fff8e1;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong> Why Split Reviews Into Sentences?</strong><br>\n",
    "  A single review can mention several animals - and express very different emotions about each one. For example:\n",
    "  <br><br>\n",
    "  <em>“The penguins were adorable! But the food was overpriced. They charged us £15 for two dry sandwiches. It's outrageous!”</em>\n",
    "  <br><br>\n",
    "  If we analyse the whole review at once, we might detect the word <code>penguins</code> and assume the reviewer had a bad experience with them. That would be misleading.\n",
    "  <br><br>\n",
    "  To avoid this, we will analyse <strong>sentiment at the sentence level</strong>, so we can link specific opinions to specific animals more reliably.\n",
    "</div>\n",
    "\n",
    "<!-- ⚙️ How Does Sentence Splitting Work? -->\n",
    "<div style=\"\n",
    "  border-left: 5px solid #007acc;\n",
    "  background: #f0f8ff;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong> How Does Sentence Splitting Work?</strong><br>\n",
    "  We'll use a pre-trained model from the <code>spaCy</code> NLP library. It’s been trained on large collections of real-world English - including books, news articles, and websites - to learn where sentences begin and end.\n",
    "  <br><br>\n",
    "  These models are fast, reliable, and much easier to use than writing custom rules by hand. Curious? You can explore spaCy's models here:  \n",
    "  <a href=\"https://spacy.io/models/en\" target=\"_blank\">https://spacy.io/models/en</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706423e8",
   "metadata": {},
   "source": [
    "#### Let's Try It Out\n",
    "\n",
    "Let's try applying the `spaCy` model to split one full review into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a full review from the dataset\n",
    "df[\"review\"].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e35d8f2",
   "metadata": {},
   "source": [
    "You should see something like:\n",
    "\n",
    "> Loved seeing all the different animals especially the penguins and giraffes! Great choice of gifts in gift shop. Staff all friendly, approachable and helpful. Despite the rain it was an enjoyable experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff986a17",
   "metadata": {},
   "source": [
    "Now let's split this into cleaner, shorter sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained English NLP model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the review text and clean it slightly\n",
    "raw_text = df[\"review\"].iloc[5]\n",
    "cleaned_text = raw_text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "# Apply the NLP model to split into sentences\n",
    "doc = nlp(cleaned_text)\n",
    "sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# Display each sentence one by one\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87e8fa",
   "metadata": {},
   "source": [
    "The output should look like:\n",
    "\n",
    "> 1. Loved seeing all the different animals especially the penguins and giraffes!\n",
    "> 2. Great choice of gifts in gift shop.\n",
    "> 3. Staff all friendly, approachable and helpful.\n",
    "> 4. Despite the rain it was an enjoyable experience!\n",
    "\n",
    "#### Why This Is Helpful\n",
    "\n",
    "This result shows how a single paragraph can be neatly split into focused thoughts:\n",
    "\n",
    "- **Sentence 1** talks about *animals* (penguins and giraffes)\n",
    "\n",
    "- **Sentence 2** is about the *gift shop*\n",
    "\n",
    "- **Sentence 3** focuses on the *staff*\n",
    "\n",
    "- **Sentence 4** reflects on the *overall experience*\n",
    "\n",
    "\n",
    "By working at the sentence level, we can now link specific opinions to specific topics - like *individual animals* - instead of just tagging a vague overall mood.\n",
    "\n",
    "#### Now Let's Do This for Every Review\n",
    "\n",
    "We've now seen how to split a single review into sentences. Next, we'll apply this to the entire dataset.\n",
    "\n",
    "This will give us:\n",
    "\n",
    "- One sentence per row\n",
    "\n",
    "- Clearer, more focused units of analysis\n",
    "\n",
    "- A better foundation for tasks like animal detection and sentiment analysis\n",
    "\n",
    "Let's build the full sentence-level dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584836e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold the results\n",
    "split_sentences = []\n",
    "\n",
    "# Loop through each review in the dataset\n",
    "for idx, review in df[\"review\"].items():\n",
    "    # Clean up formatting\n",
    "    cleaned = review.replace(\"\\n\", \" \").strip()\n",
    "    \n",
    "    # Apply the NLP model\n",
    "    doc = nlp(cleaned)\n",
    "    \n",
    "    # Extract individual sentences\n",
    "    for sentence in doc.sents:\n",
    "        split_sentences.append({\n",
    "            \"Review Index\": idx,\n",
    "            \"Sentence\": sentence.text.strip()\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame with one sentence per row\n",
    "sentences_df = pd.DataFrame(split_sentences)\n",
    "\n",
    "# Show a preview of the result\n",
    "sentences_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db9fcc",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Find the Animals!\n",
    "---\n",
    "\n",
    "Now that each review is split into individual sentences, we can look more closely at **what** people are talking about.\n",
    "\n",
    "In this step, we'll use a technique called **Named Entity Recognition (NER)** to detect specific animals mentioned in the text - like *penguins*, *giraffes*, or *tigers*.\n",
    "\n",
    "NER is part of a field called **Natural Language Processing (NLP)**. It's used to automatically identify and label important things in text, such as:\n",
    "\n",
    "- People's names (e.g. *Jane Goodall*)\n",
    "- Locations (e.g. *Edinburgh Zoo*)\n",
    "- Organisations (e.g. *RZSS*)\n",
    "- Products, dates, times…  \n",
    "...and yes, even **animals**.\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 5px solid #007acc;\n",
    "  background: #f0f8ff;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong> What's the Point?</strong><br>\n",
    "  By tagging animal mentions in each sentence, we'll be able to:\n",
    "  <ul style=\"margin-top: 0.5em; margin-bottom: 0;\">\n",
    "    <li>Count which animals are most talked about</li>\n",
    "    <li>Link animals to specific opinions and emotions</li>\n",
    "    <li>Create fun visualisations of animal mentions</li>\n",
    "  </ul>\n",
    "  <p style=\"margin-top: 0.75em; margin-bottom: 0;\">\n",
    "    This helps us understand how visitors feel about different parts of the zoo.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "Next, we'll load a pre-trained NER model (like the one from <code>spaCy</code>), run it on our sentence-level DataFrame, and extract any animals it can find.\n",
    "\n",
    "Later, we'll combine this with **sentiment analysis** to detect *how* people feel about each animal.  \n",
    "\n",
    "Let's get ready to go animal spotting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be89de9",
   "metadata": {},
   "source": [
    "#### Let's see how it works under the hood!\n",
    "\n",
    "The model has already been trained, and we can load it using the `transformers` library. To do this, we need two components: the **tokenizer** and the **model** itself.  \n",
    "\n",
    "These are loaded in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dadb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules to load the pre-trained NER model\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Specify the path of the folder where the model and tokenizer are stored\n",
    "LOAD_MODEL_PATH = \"/afs/inf.ed.ac.uk/group/project/suttontrust/sutton_workshop/animal-ner-model\"\n",
    "\n",
    "# Load the pre-trained NER model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOAD_MODEL_PATH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(LOAD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a9897",
   "metadata": {},
   "source": [
    "Now we need to prepare the sentences in our dataset so they can be passed into the model. But we can't do that directly — the model doesn't actually understand plain text.  \n",
    "\n",
    "Instead, we must first **tokenise** the sentences: that is, break them down into smaller units called *sub-words*.  \n",
    "\n",
    "To do this, we'll use a function called `tokenize_sentences`. Let’s see how it works on an example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40674d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a sentence to tokenize\n",
    "# Change this sentence to whatever you want!\n",
    "example_sent = \"We loved to see the tigers and penguins at the zoo.\"\n",
    "\n",
    "# Tokenize the example sentence\n",
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of sentences and returns the encodings.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        sentences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "encodings = tokenize_sentences([example_sent])\n",
    "\n",
    "# Get the information we need out of the encodings\n",
    "input_ids = encodings[\"input_ids\"]\n",
    "attention_mask = encodings[\"attention_mask\"]\n",
    "offset_mappings = encodings[\"offset_mapping\"]\n",
    "\n",
    "# Use the encodings information to annotate the sentence\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Use the function \"argmax\" to extract the predictions from the model's output\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Convert predictions to labels\n",
    "labels = [model.config.id2label[pred_id.item()] for pred_id in predictions[0]]\n",
    "\n",
    "# Show the results\n",
    "print(\"Original sentence:\", example_sent)\n",
    "\n",
    "print(\"Result:\")\n",
    "pd.DataFrame({\n",
    "    \"Token\":            tokenizer.convert_ids_to_tokens(encodings[\"input_ids\"][0]),\n",
    "    \"Input IDs\":        input_ids[0].tolist(),\n",
    "    \"Offset Mapping\":   offset_mappings[0].tolist(),\n",
    "    \"Predictions\":      predictions[0].tolist(),\n",
    "    \"Predicted Label\":  labels\n",
    "}).set_index(\"Token\").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d0c1b",
   "metadata": {},
   "source": [
    "You may have noticed something interesting in the table above...\n",
    "\n",
    "Tokenising doesn't always mean splitting text into whole words. For example, the word *\"penguins\"* is tokenised into three smaller parts (called *tokens*): `\"pen\"`, `\"##guin\"`, and `\"##s\"`.\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 5px solid #2196f3;\n",
    "  background: #e3f2fd;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong>What's a token, exactly?</strong><br>\n",
    "  A <em>token</em> is the basic unit that the model understands — and in modern models like this one, most tokens are actually <strong>subwords</strong>.<br><br>\n",
    "  That means a single word like <code>penguins</code> might get split into multiple tokens: <code>\"pen\"</code>, <code>\"##guin\"</code>, and <code>\"##s\"</code>.<br><br>\n",
    "  So:\n",
    "  <ul style=\"margin-top: 0.25em; margin-bottom: 0;\">\n",
    "    <li><strong>All subwords are tokens</strong></li>\n",
    "    <li><strong>But not all tokens are full words!</strong></li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "The `\"##\"` symbol means that the token continues from the previous one — it's part of the same word.\n",
    "\n",
    "Why does this happen? The model has a limited vocabulary of known sub-word units. Rare or unusual words are broken down into smaller chunks, while common words are often kept whole.\n",
    "\n",
    "That's why *\"tigers\"* (a more common word) becomes just `\"tiger\"` and `\"##s\"`, while *\"penguins\"* is split into three tokens.\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 5px solid #4caf50;\n",
    "  background: #f6ffed;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong>Try it yourself:</strong><br>\n",
    "  Change the sentence in <code>example_sent</code> above to something new, and see how the tokenisation changes!  \n",
    "  Can you find a rare or long word that gets split into lots of tokens?  \n",
    "  What about an animal that's only partly recognised?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d4999",
   "metadata": {},
   "source": [
    "#### Use the model on all sentences in the dataset\n",
    "\n",
    "The next cell contains all the code needed to:\n",
    "- get predictions from the model (just like we did above), and  \n",
    "- convert detected animal tokens (like `pen`, `##guin`, `##s`) back into the full word (`penguins`).\n",
    "\n",
    "You're welcome to explore the code if you're curious — but there's no need to understand how it works!\n",
    "\n",
    "Running the model on *all* sentences in the dataset would take a long time on this computer.  \n",
    "**Why?** Because this machine doesn't have access to a **GPU** (*Graphics Processing Unit*) — a special chip (like a graphics card in a gaming PC) that can perform lots of calculations at once.\n",
    "\n",
    "Machine learning models (like the one we're using to detect animal names) run much faster on a GPU. Without one, everything slows down — like trying to play a high-end video game on an old laptop.\n",
    "\n",
    "To keep things quick, we'll just run the model on 500 sentences (just to show it works!). After that, we'll load a pre-processed version of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry if this looks complicated — this part of the code is here to help the model\n",
    "# figure out where the animal names are in each sentence. It’s been written for you!\n",
    "\n",
    "# === Annotate a subset of 500 review sentences\n",
    "from tqdm import tqdm  # Used to create a progress bar\n",
    "import torch\n",
    "\n",
    "# === Helper functions\n",
    "def flush_entity(entities, entity_token_idxs, offsets, sentence):\n",
    "    \"\"\"\n",
    "    Save the entity into the 'entities' list.\n",
    "    \"\"\"\n",
    "    # Get character-level start and end positions of the whole entity\n",
    "    span_offsets = [offsets[i].tolist() for i in entity_token_idxs]\n",
    "    start_char = span_offsets[0][0]\n",
    "    end_char = span_offsets[-1][1]\n",
    "\n",
    "    # Save the entity in the list as a tuple: (animal name, start position, end position)\n",
    "    new_entity = (sentence[start_char:end_char].strip().lower(), start_char, end_char)\n",
    "\n",
    "    # Avoid duplicates (e.g. in cases like to ##rt ##oise where tokens are inconsistently labelled)\n",
    "    if new_entity not in entities:\n",
    "        entities.append(new_entity)\n",
    "\n",
    "    # Reset the tracker for the next entity\n",
    "    entity_token_idxs = []\n",
    "\n",
    "    return entities, entity_token_idxs\n",
    "\n",
    "def backtrack_to_word_start(input_ids_sent, tok_idx, entity_token_idxs):\n",
    "    \"\"\"Backtracks from a subword token to the start of the full word.\"\"\"\n",
    "\n",
    "    # If the initial token is a subword, search for the start of the word\n",
    "    token_text = tokenizer.convert_ids_to_tokens([input_ids_sent[tok_idx]])[0]\n",
    "    if token_text.startswith(\"##\"):\n",
    "\n",
    "        # Move backwards to find the start of the word\n",
    "        prev_idx = tok_idx - 1\n",
    "\n",
    "        while prev_idx >= 0:\n",
    "            entity_token_idxs.insert(0, prev_idx)\n",
    "\n",
    "            token_text = tokenizer.convert_ids_to_tokens([input_ids_sent[prev_idx]])[0]\n",
    "            if token_text.startswith(\"##\"):\n",
    "                prev_idx -= 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return entity_token_idxs\n",
    "\n",
    "def forwardtrack_to_word_end(input_ids_sent, entity_token_idxs):\n",
    "    \"\"\"Moves forward to include subword tokens that are part of the same word (e.g. ##e from giraffe).\"\"\"\n",
    "\n",
    "    next_idx = entity_token_idxs[-1] + 1\n",
    "\n",
    "    while next_idx < len(input_ids_sent):\n",
    "        next_token_text = tokenizer.convert_ids_to_tokens([input_ids_sent[next_idx]])[0]\n",
    "\n",
    "        if next_token_text.startswith(\"##\"):\n",
    "            entity_token_idxs.append(next_idx)\n",
    "            next_idx += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return entity_token_idxs\n",
    "\n",
    "def get_entities_from_predictions(preds, input_ids_sent, offsets):\n",
    "    \"\"\"\n",
    "    Extracts entities from the model’s predictions and input IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    entity_token_idxs = []  # Holds indices of tokens currently forming an entity\n",
    "    entities = []           # Will store all extracted animal mentions for the sentence\n",
    "    label = \"O\"             # Initial label (O = Outside entity)\n",
    "\n",
    "    for tok_idx, pred_id in enumerate(preds):\n",
    "        previous_label = label\n",
    "        label = model.config.id2label[pred_id.item()]\n",
    "        start, end = offsets[tok_idx].tolist()\n",
    "\n",
    "        # Skip special tokens ([CLS], [SEP], padding)\n",
    "        if start == end == 0:\n",
    "            continue\n",
    "\n",
    "        # Skip non-entity tokens\n",
    "        if label == \"O\":\n",
    "            continue\n",
    "\n",
    "        # Correct cases of I-ANIMAL without a preceding B-ANIMAL\n",
    "        if (label == \"I-ANIMAL\") and (previous_label not in {\"B-ANIMAL\", \"I-ANIMAL\"}):\n",
    "            label = \"B-ANIMAL\"\n",
    "\n",
    "        # If we’ve already been tracking an entity and this token is not part of it,\n",
    "        # save the entity before starting a new one\n",
    "        if (entity_token_idxs != []) and (label != \"I-ANIMAL\"):\n",
    "            entity_token_idxs = forwardtrack_to_word_end(input_ids_sent, entity_token_idxs)\n",
    "            entities, entity_token_idxs = flush_entity(entities, entity_token_idxs, offsets, sentence)\n",
    "\n",
    "        if label == \"B-ANIMAL\":\n",
    "            entity_token_idxs = [tok_idx]\n",
    "            entity_token_idxs = backtrack_to_word_start(input_ids_sent, tok_idx, entity_token_idxs)\n",
    "\n",
    "        elif label == \"I-ANIMAL\":\n",
    "            entity_token_idxs.append(tok_idx)\n",
    "\n",
    "    # Final check in case sentence ends with an entity\n",
    "    if entity_token_idxs:\n",
    "        entities, entity_token_idxs = flush_entity(entities, entity_token_idxs, offsets, sentence)\n",
    "\n",
    "    animals = [ent[0] for ent in entities]\n",
    "    spans = [tuple(ent[1:3]) for ent in entities]\n",
    "\n",
    "    return animals, spans\n",
    "\n",
    "# === Extract the animals!\n",
    "\n",
    "# \"Batch size\" refers to how many sentences we process at once.\n",
    "# Processing more at once is faster, but uses more memory.\n",
    "batch_size = 16\n",
    "\n",
    "# Initialise a list to store the annotated sentences\n",
    "NER_animals = []\n",
    "\n",
    "# Convert the DataFrame sentences into a list\n",
    "all_sentences = list(sentences_df[\"Sentence\"])\n",
    "\n",
    "# Only run on a subset of sentences to save time (running on all would take ~5 minutes)\n",
    "all_sentences = all_sentences[:512]\n",
    "\n",
    "# Use tqdm to show progress while we find animals in each batch\n",
    "for i in tqdm(range(0, len(all_sentences), batch_size), desc=\"Finding animals in text\"):\n",
    "    sentences = all_sentences[i:i+batch_size]\n",
    "\n",
    "    # Tokenise the sentences in the batch\n",
    "    encodings = tokenize_sentences(sentences)\n",
    "\n",
    "    input_ids = encodings[\"input_ids\"]\n",
    "    attention_mask = encodings[\"attention_mask\"]\n",
    "    offset_mappings = encodings[\"offset_mapping\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    for sent_idx, sentence in enumerate(sentences):\n",
    "        preds = predictions[sent_idx]\n",
    "        input_ids_sent = input_ids[sent_idx]\n",
    "        offsets = offset_mappings[sent_idx]\n",
    "\n",
    "        # Extract animals and character spans\n",
    "        animals, spans = get_entities_from_predictions(preds, input_ids_sent, offsets)\n",
    "\n",
    "        NER_animals.append({\n",
    "            \"review_sentence\": sentence,\n",
    "            \"animals\": animals,\n",
    "            \"spans\": spans\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for easier use\n",
    "animal_sentences_df = pd.DataFrame(NER_animals)\n",
    "\n",
    "# The above only ran on a small subset — now load the full version (pre-processed)\n",
    "animal_sentences_df = pd.read_json(\"NER_animals.json\")\n",
    "\n",
    "# Filter out any rows where no animals were detected\n",
    "animal_sentences_df = animal_sentences_df[animal_sentences_df[\"animals\"].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Show a few examples\n",
    "animal_sentences_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb912744",
   "metadata": {},
   "source": [
    "Now that we have the annotated dataset, let's take a look at how many different animal mentions it found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each animal was mentioned across all sentences\n",
    "animals_mention_counts = (\n",
    "    animal_sentences_df.explode(\"animals\")[\"animals\"]\n",
    "    .value_counts()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# How many different animals were mentioned?\n",
    "print(f\"Number of different animals mentioned: {len(animals_mention_counts)}\")\n",
    "\n",
    "# Show the top 10 most frequently mentioned animals\n",
    "animals_mention_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f550ef",
   "metadata": {},
   "source": [
    "The Edinburgh Zoo doesn't have *that* many animals! What's going on?\n",
    "\n",
    "Well, people often refer to the same animal in different ways. For example, `pandas`, `panda`, and `giant pandas` might all refer to the same species.\n",
    "\n",
    "The process of grouping these different variations into a single, standard form is called **normalisation** – and that's what we'll do next!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19aaad4",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Normalise the animal names!\n",
    "---\n",
    "\n",
    "Let's take a closer look at how people refer to the same animal in different ways.  \n",
    "\n",
    "For example, how many different ways do people talk about penguins? We'll use a quick shortcut: searching for all detected entities that contain `peng` in their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935edc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the string you want to search for (e.g., part of an animal name)\n",
    "string_to_search = \"peng\"\n",
    "\n",
    "# Filter the list of animal mentions to include only those containing the search string\n",
    "# This is case-insensitive, so it will match 'Penguin', 'penguins', 'Emperor Penguin', etc.\n",
    "filtered_animals = animals_mention_counts[animals_mention_counts.index.str.contains(string_to_search, case=False)]\n",
    "\n",
    "# Show the results\n",
    "filtered_animals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2fbedc",
   "metadata": {},
   "source": [
    "As you can see, people refer to `penguins` in lots of different ways.  \n",
    "We need a way to recognise that all these mentions point to the same zoo animal!\n",
    "\n",
    "What we've done just now is a simple **string match**.  \n",
    "That means the computer is looking for the exact sequence of letters you typed — nothing more, nothing less.  \n",
    "\n",
    "So if you search for `\"peng\"`, it will only find words that contain those exact letters in that order.  \n",
    "It won't catch things like `\"pinguin\"` (a possible typo), or `\"penguin exhibit\"` if the entity was tokenised strangely.\n",
    "\n",
    "We need a smarter way to group together different spellings, formats, and even occasional misspellings of the same animal name.  \n",
    "This process is called **normalising** the entities.\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 5px solid #4caf50;\n",
    "  background: #f6ffed;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong>Try this:</strong><br>\n",
    "  Change the <code>string_to_search</code> to search for other animals!  \n",
    "  For example: <code>\"cat\"</code>, <code>\"giraf\"</code>, <code>\"bear\"</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902fbdb",
   "metadata": {},
   "source": [
    "#### Our Custom Animal List\n",
    "\n",
    "We've put together a list of animals currently found at **Edinburgh Zoo**,  \n",
    "along with common ways visitors might refer to them.\n",
    "\n",
    "This includes:\n",
    "\n",
    "- Official species names (e.g. `\"Northern rockhopper penguin\"`)\n",
    "- Simpler terms (e.g. `\"penguin\"`, `\"giraffe\"`, `\"tiger\"`)\n",
    "- Nicknames and informal aliases (e.g. `\"bearcat\"` for a *binturong*, `\"duck\"` for a *pochard*)\n",
    "\n",
    "You can see the full list in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9656b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIAS DICTIONARY\n",
    "edinburgh_zoo_animal_aliases = {\n",
    "    \"Alpaca\": [\"alpaca\"],\n",
    "    \"Asian small clawed otter\": [\"asian otter\", \"small clawed otter\", \"otter\"],\n",
    "    \"Asiatic lion\": [\"asiatic lion\", \"lion\"],\n",
    "    \"Baer's pochard\": [\"pochard\", \"baer's pochard\", \"duck\"],\n",
    "    \"Bagot goat\": [\"bagot goat\", \"goat\"],\n",
    "    \"Banteng\": [\"banteng\", \"wild cattle\"],\n",
    "    \"Binturong\": [\"binturong\", \"bearcat\"],\n",
    "    \"Blue poison dart frog\": [\"blue poison dart frog\", \"poison dart frog\", \"dart frog\", \"frog\"],\n",
    "    \"Brown capuchin\": [\"brown capuchin\", \"capuchin monkey\", \"capuchin\"],\n",
    "    \"Buff-cheeked gibbon\": [\"buff-cheeked gibbon\", \"gibbon\"],\n",
    "    \"Cape porcupine\": [\"cape porcupine\", \"porcupine\"],\n",
    "    \"Capybara\": [\"capybara\", \"capy\", \"capys\"],\n",
    "    \"Cheetah\": [\"cheetah\"],\n",
    "    \"Chilean flamingo\": [\"chilean flamingo\", \"flamingo\"],\n",
    "    \"Chilean rose tarantula\": [\"chilean rose tarantula\", \"tarantula\", \"spider\"],\n",
    "    \"Chimpanzee\": [\"chimpanzee\", \"chimp\", \"chimps\"],\n",
    "    \"Chinese goral\": [\"chinese goral\", \"goral\"],\n",
    "    \"Common squirrel monkey\": [\"common squirrel monkey\", \"squirrel monkey\"],\n",
    "    \"Coppery titi monkey\": [\"coppery titi monkey\", \"titi monkey\"],\n",
    "    \"Cotton-top tamarin\": [\"cotton-top tamarin\", \"tamarin\"],\n",
    "    \"Crowned lemur\": [\"crowned lemur\", \"lemur\"],\n",
    "    \"Drill\": [\"drill\"],\n",
    "    \"East African crowned crane\": [\"east african crowned crane\", \"crowned crane\", \"crane\"],\n",
    "    \"Eastern white pelican\": [\"eastern white pelican\", \"pelican\"],\n",
    "    \"Gelada baboon\": [\"gelada\", \"gelada baboon\", \"baboon\"],\n",
    "    \"Gentoo penguin\": [\"gentoo penguin\", \"penguin\"],\n",
    "    \"Giant African land snail\": [\"giant african land snail\", \"land snail\", \"snail\"],\n",
    "    \"Giant anteater\": [\"giant anteater\", \"anteater\"],\n",
    "    \"Goeldi's monkey\": [\"goeldi's monkey\", \"goeldi monkey\"],\n",
    "    \"Greater one-horned rhinoceros\": [\"greater one-horned rhinoceros\", \"rhinoceros\", \"rhino\"],\n",
    "    \"Grevy's zebra\": [\"grevy's zebra\", \"zebra\"],\n",
    "    \"Ground cuscus\": [\"ground cuscus\", \"cuscus\"],\n",
    "    \"Japanese macaque\": [\"japanese macaque\", \"macaque\"],\n",
    "    \"King penguin\": [\"king penguin\", \"penguin\"],\n",
    "    \"Kirk's dik-dik\": [\"kirk's dik-dik\", \"dik-dik\"],\n",
    "    \"L'Hoest's monkey\": [\"lhoest's monkey\"],\n",
    "    \"Land hermit crab\": [\"land hermit crab\", \"hermit crab\", \"crab\"],\n",
    "    \"Large hairy armadillo\": [\"large hairy armadillo\", \"armadillo\"],\n",
    "    \"Leopard tortoise\": [\"leopard tortoise\", \"tortoise\"],\n",
    "    \"Linne's two-toed sloth\": [\"linne's two-toed sloth\", \"two-toed sloth\", \"sloth\"],\n",
    "    \"Lowland nyala\": [\"lowland nyala\", \"nyala\"],\n",
    "    \"Malayan sun bear\": [\"malayan sun bear\", \"sun bear\", \"bear\"],\n",
    "    \"Meerkat\": [\"meerkat\"],\n",
    "    \"Northern Luzon giant cloud rat\": [\"northern luzon giant cloud rat\", \"cloud rat\", \"rat\"],\n",
    "    \"Northern rockhopper penguin\": [\"northern rockhopper penguin\", \"rockhopper penguin\", \"penguin\"],\n",
    "    \"Nubian giraffe\": [\"nubian giraffe\", \"giraffe\"],\n",
    "    \"Pallas's cat\": [\"pallas's cat\", \"pallas cat\"],\n",
    "    \"Pied imperial pigeon\": [\"pied imperial pigeon\", \"pigeon\"],\n",
    "    \"Prevost's squirrel\": [\"prevost's squirrel\", \"squirrel\"],\n",
    "    \"Przewalski's wild horse\": [\"przewalski's wild horse\", \"wild horse\", \"horse\"],\n",
    "    \"Pygmy hippo\": [\"pygmy hippo\", \"hippopotamus\", \"hippo\"],\n",
    "    \"Queensland koala\": [\"queensland koala\", \"koala\"],\n",
    "    \"Red panda\": [\"red panda\"],\n",
    "    \"Giant panda\": [\"giant\", \"panda\", \"giant panda\"],\n",
    "    \"Red river hog\": [\"red river hog\", \"hog\", \"pig\"],\n",
    "    \"Red-bellied lemur\": [\"red-bellied lemur\", \"lemur\"],\n",
    "    \"Red-fronted macaw\": [\"red-fronted macaw\", \"macaw\", \"parrot\"],\n",
    "    \"Reef tank\": [\"reef tank\", \"reef\", \"aquarium\"],\n",
    "    \"Ring-tailed lemur\": [\"ring-tailed lemur\", \"lemur\"],\n",
    "    \"Southern cassowary\": [\"southern cassowary\", \"cassowary\"],\n",
    "    \"Southern pudu\": [\"southern pudu\", \"pudu\"],\n",
    "    \"Southern three-banded armadillo\": [\"southern three-banded armadillo\", \"three-banded armadillo\", \"armadillo\"],\n",
    "    \"Sumatran tiger\": [\"sumatran tiger\", \"tiger\"],\n",
    "    \"Sun beetle\": [\"sun beetle\", \"beetle\"],\n",
    "    \"Swamp wallaby\": [\"swamp wallaby\", \"wallaby\"],\n",
    "    \"Turkmenian markhor\": [\"turkmenian markhor\", \"markhor\", \"goat\"],\n",
    "    \"Visayan spotted deer\": [\"visayan spotted deer\", \"spotted deer\", \"deer\"],\n",
    "    \"Visayan warty pig\": [\"visayan warty pig\", \"warty pig\", \"pig\"],\n",
    "    \"Waldrapp ibis\": [\"waldrapp ibis\", \"ibis\"],\n",
    "    \"Western grey kangaroo\": [\"western grey kangaroo\", \"kangaroo\"],\n",
    "    \"Wildcat\": [\"wildcat\", \"cat\"],\n",
    "    \"Monkeys\": [\"monkey\"] # General category for unspecified monkey mentions\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ce822",
   "metadata": {},
   "source": [
    "By checking each of the detected animal names against our list of aliases, we can automatically group all the different ways people refer to the same zoo animal.\n",
    "\n",
    "But it's not always that simple.\n",
    "\n",
    "People often make **typos** in their reviews — like writing `\"pengvins\"` instead of `\"penguins\"` — or they might describe animals in unusual ways. So we need a smarter way to figure out what they’re actually talking about.\n",
    "\n",
    "Here's how we handle it:\n",
    "1. **Fix typos** using an algorithm that checks whether a word is *close enough* to something in our list (e.g. `\"pinguin\"` → `\"penguin\"`).\n",
    "2. **Lemmatise** the animal names — this means converting words into their simplest form.  \n",
    "   For example, `\"penguins\"` becomes `\"penguin\"`, so that plurals and slight variations are grouped together.\n",
    "3. **Check for partial matches**, such as mapping `\"lion cub\"` to `\"lion\"`.\n",
    "\n",
    "Once we've done all that, we use our alias dictionary to map each cleaned-up name to its proper **species** — for example:  \n",
    "`\"penguin\"` → `\"Northern rockhopper penguin\"`  \n",
    "`\"giraffe\"` → `\"Nubian giraffe\"`\n",
    "\n",
    "This gives us a clean, consistent list of species for each sentence — no matter how the animals were originally described.  \n",
    "That way, we can clearly understand what visitors are saying about each animal in the zoo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def normalise_entity(entity: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Use spaCy to convert each word in the entity to its base form (lemma).\n",
    "    \"\"\"\n",
    "    entity = entity.strip()\n",
    "\n",
    "    # Return None for empty strings\n",
    "    if entity == \"\":\n",
    "        return None\n",
    "    \n",
    "    # Lemmatise each word and convert to lowercase\n",
    "    doc = nlp(entity)\n",
    "    norm_entity = \" \".join([doc[i].lemma_.lower() for i in range(len(doc))])\n",
    "\n",
    "    return norm_entity\n",
    "\n",
    "from rapidfuzz.distance import JaroWinkler\n",
    "\n",
    "def correct_misspelled_animal(misspelled: str, aliases: set, threshold=0.90):\n",
    "    \"\"\"\n",
    "    Use Jaro-Winkler similarity to correct misspellings.\n",
    "    Returns the best match if it is similar enough (above threshold).\n",
    "    \"\"\"\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    for animal in aliases:\n",
    "        score = JaroWinkler.similarity(misspelled, animal)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = animal\n",
    "\n",
    "    return best_match if best_score >= threshold else None\n",
    "\n",
    "def map_to_species(norm_animal: str, normalised_animals_mapping: dict) -> str | None:\n",
    "    \"\"\"\n",
    "    Attempt to map the normalised entity to a zoo species using:\n",
    "    1. Direct alias matching\n",
    "    2. Word-level partial matching (e.g. 'lion cub' → 'lion')\n",
    "    3. Typo correction using fuzzy matching\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Exact match to a known alias\n",
    "    if norm_animal in normalised_animals_mapping:\n",
    "        return normalised_animals_mapping[norm_animal]\n",
    "    \n",
    "    # Step 2: Partial match (any word in the phrase matches an alias)\n",
    "    for word in norm_animal.split(\" \"):\n",
    "        if word in normalised_animals_mapping:\n",
    "            return normalised_animals_mapping[word]\n",
    "\n",
    "    # Step 3: Fuzzy match for likely misspellings\n",
    "    species = correct_misspelled_animal(norm_animal, set(normalised_animals_mapping.keys()))\n",
    "    if species:\n",
    "        return normalised_animals_mapping[species]\n",
    "    \n",
    "    # Step 4: No match found\n",
    "    return None\n",
    "\n",
    "# Create a mapping from each normalised alias to the correct species name\n",
    "normalised_animals_mapping = {\n",
    "    normalise_entity(alias): species\n",
    "    for species, aliases in edinburgh_zoo_animal_aliases.items()\n",
    "    for alias in aliases\n",
    "}\n",
    "\n",
    "# Apply lemmatisation to each detected animal name\n",
    "animal_sentences_df[\"norm_animals\"] = animal_sentences_df[\"animals\"].apply(\n",
    "    lambda x: [normalise_entity(ent) for ent in x]\n",
    ")\n",
    "\n",
    "# Map each normalised animal to the zoo species\n",
    "animal_sentences_df[\"species\"] = animal_sentences_df[\"norm_animals\"].apply(\n",
    "    lambda x: [map_to_species(norm_animal, normalised_animals_mapping) for norm_animal in x]\n",
    ")\n",
    "\n",
    "# Preview the results\n",
    "animal_sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f2471",
   "metadata": {},
   "source": [
    "And with that, the sentences are now normalised! Let's check the penguins again — have we managed to group all their different mentions under the same species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the species you want to search for\n",
    "species = \"Northern rockhopper penguin\"\n",
    "\n",
    "# Find all sentences that mention this species\n",
    "filtered_sentences = animal_sentences_df[animal_sentences_df[\"species\"].apply(lambda x: species in x)]\n",
    "\n",
    "print(f\"Found {len(filtered_sentences)} sentences mentioning the species '{species}'. Here are 5 examples:\")\n",
    "filtered_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438baa90",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  border-left: 5px solid #4caf50;\n",
    "  background: #f6ffed;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong>Try it yourself!</strong>  \n",
    "  Search for other animals — can you spot any cases where the normalisation hasn't worked quite perfectly?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a5fa5",
   "metadata": {},
   "source": [
    "This normalisation approach uses **traditional methods** — things like string matching and rule-based algorithms.  \n",
    "\n",
    "Nowadays, more advanced methods use **large language models (LLMs)** to perform normalisation. You can even train a model to learn the best way to group or correct animal names based on patterns in data.\n",
    "\n",
    "However, that kind of approach requires a large labelled dataset — something we didn't have the time or resources to build.\n",
    "\n",
    "Luckily, our simpler method works pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6d326",
   "metadata": {},
   "source": [
    "Finally, let’s take a look at the animal mentions we've **not** been able to normalise.\n",
    "\n",
    "So far, we've only been able to normalise animals that are on our list — that is, animals currently at the zoo. But if someone used a completely different term (like `\"primates\"` instead of `\"monkey\"`), mentioned an animal not found at the zoo (e.g. `\"elephant\"`), or if something was mistakenly annotated as an animal (like `\"joey\"`), then those names won't have been normalised.\n",
    "\n",
    "Let's look at the 20 most common animal mentions that couldn't be mapped to a zoo species.  \n",
    "\n",
    "This can give us insight into the **limitations of the model** — or perhaps even the zoo itself! *Is there an animal that visitors talk about a lot, but isn't in the zoo?* *Should the zoo consider including it?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode list columns so each animal mention is in its own row\n",
    "all_animals = animal_sentences_df.explode([\"animals\", \"spans\", \"norm_animals\", \"species\"])\n",
    "\n",
    "# Filter for rows where no species was assigned (i.e., normalisation failed)\n",
    "unnormalised_animals = (\n",
    "    all_animals[all_animals[\"species\"].isna()][\"norm_animals\"]\n",
    "    .value_counts()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Print the top 20 unnormalised mentions\n",
    "unnormalised_animals.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1759c6e",
   "metadata": {},
   "source": [
    "You can now investigate any particular entity that has not been normalised. Change the string in the next cell, and it will print all the sentences that have not been normalised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the animal to investigate\n",
    "norm_animal = \"cub\"\n",
    "\n",
    "# Show 10 sentences when that animal is mentioned\n",
    "animal_sentences_df[animal_sentences_df[\"norm_animals\"].apply(lambda x: norm_animal in x)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b5b2b",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  border-left: 5px solid #2196f3;\n",
    "  background: #e3f2fd;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong>Question 1:</strong> What is the most frequently mentioned animal that isn't in the zoo?  \n",
    "  What do visitors say about it?\n",
    "</div>\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 5px solid #2196f3;\n",
    "  background: #e3f2fd;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong>Question 2:</strong> What are the limitations of the model?  \n",
    "  Can you find any sentences where the model has mistakenly labelled something as an animal when it isn't?  \n",
    "  Why do you think that happened? What might have confused the model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90823948",
   "metadata": {},
   "source": [
    "You’ve now:\n",
    "\n",
    "- Broken the reviews into individual sentences  \n",
    "- Matched those sentences to specific animals using a smart alias system  \n",
    "- Filtered the dataset to include only the sentences that mention animals\n",
    "\n",
    "This gives us a clean set of **animal-related sentences** — perfect for analysis.\n",
    "\n",
    "But before we dive into how people feel about the animals, let’s pause and ask:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d890ac",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Which Animals Are Talked About the Most?\n",
    "---\n",
    "\n",
    "Some animals might be mentioned constantly. Others only occasionally.\n",
    "\n",
    "By counting mentions, we can:\n",
    "\n",
    "- Spot the zoo's most attention-grabbing species\n",
    "\n",
    "- Set expectations for the sentiment results (do popular animals get more love… or more complaints?)\n",
    "\n",
    "- Identify which animals are worth focusing on in further analysis\n",
    "\n",
    "Let's visualise which animals come up the most in reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ae1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count number of times each animal was mentioned (including duplicates per sentence)\n",
    "mention_counts = (\n",
    "    animal_sentences_df.explode(\"species\")[\"species\"]\n",
    "    .value_counts()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Plot top 15\n",
    "mention_counts.head(15).plot(\n",
    "    kind=\"barh\",\n",
    "    figsize=(10, 6),\n",
    "    color=\"#3498db\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Mentions\")\n",
    "plt.ylabel(\"Animal\")\n",
    "plt.title(\"Top 15 Most Frequently Mentioned Animals in Reviews\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee33c7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: How Do People Feel About Each Animal?\n",
    "---\n",
    "\n",
    "Now that we know **which animals are mentioned most**, let's go one step further: we want to understand the **emotion or tone** behind each sentence.\n",
    "\n",
    "Was the visitor...\n",
    "\n",
    "- Excited to see the giraffes?\n",
    "\n",
    "- Indifferent about the meerkats?\n",
    "\n",
    "- Annoyed that the penguins weren't visible?\n",
    "\n",
    "This process is called **sentiment analysis** - a technique that uses Natural Language Processing (NLP) to automatically detect the **emotional tone** of a sentence or phrase.\n",
    "\n",
    "We'll apply sentiment analysis to each animal-related sentence. This will allow us to:\n",
    "\n",
    "- Measure which animals get the most *positive* or *negative* feedback\n",
    "\n",
    "- Spot *controversial* animals with mixed reactions\n",
    "\n",
    "- Build visualisations to summarise how the zoo's animals are perceived overall\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8184628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load sentiment model ===\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create pipeline with batch processing enabled\n",
    "sentiment_model = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=32,  # Process 32 sentences at once\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# === Run Sentiment Analysis on Sentences that Mention Animals ===\n",
    "\n",
    "def get_sentiment_batch(texts):\n",
    "    \"\"\"Process multiple texts at once for better efficiency\"\"\"\n",
    "    try:\n",
    "        # Process all texts in batch\n",
    "        outputs = sentiment_model(texts)\n",
    "        \n",
    "        # Map labels to readable format\n",
    "        label_map = {\n",
    "            \"LABEL_0\": \"negative\",\n",
    "            \"LABEL_1\": \"neutral\", \n",
    "            \"LABEL_2\": \"positive\"\n",
    "        }\n",
    "        \n",
    "        results = []\n",
    "        for output in outputs:\n",
    "            results.append({\n",
    "                \"Sentiment\": label_map[output[\"label\"]],\n",
    "                \"Sentiment Score\": output[\"score\"]\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    except Exception as e:\n",
    "        # Return error results for the batch\n",
    "        return pd.DataFrame([{\n",
    "            \"Sentiment\": \"error\",\n",
    "            \"Sentiment Score\": 0.0\n",
    "        }] * len(texts))\n",
    "\n",
    "# Process sentences in batches with progress bar\n",
    "batch_size = 32\n",
    "sentences = animal_sentences_df[\"review_sentence\"].tolist()\n",
    "all_results = []\n",
    "\n",
    "print(\"Running sentiment analysis on animal-related sentences...\")\n",
    "\n",
    "# Process in batches with tqdm progress bar\n",
    "for i in tqdm(range(0, len(sentences), batch_size), desc=\"Processing batches\"):\n",
    "    batch = sentences[i:i+batch_size]\n",
    "    batch_results = get_sentiment_batch(batch)\n",
    "    all_results.append(batch_results)\n",
    "\n",
    "# Combine all results\n",
    "sentiment_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Add results to the DataFrame\n",
    "animal_sentences_df[[\"Sentiment\", \"Sentiment Score\"]] = sentiment_results\n",
    "\n",
    "# Preview the result\n",
    "print(f\"\\nCompleted sentiment analysis on {len(animal_sentences_df)} sentences!\")\n",
    "animal_sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312622a",
   "metadata": {},
   "source": [
    "#### What Did We Learn from the Sentiment Scores?\n",
    "\n",
    "We now have a powerful dataset!\n",
    "Each sentence is tagged with:\n",
    "\n",
    "- The **animals** it mentions\n",
    "\n",
    "- Whether it was **positive**, **neutral**, or **negative**\n",
    "\n",
    "- A **confidence score** between 0 and 1\n",
    "\n",
    "This sets us up for some really interesting questions:\n",
    "\n",
    "- Which animals got the *happiest* reviews?\n",
    "\n",
    "- Which ones triggered complaints or mixed feelings?\n",
    "\n",
    "- How does public opinion vary between species?\n",
    "\n",
    "To answer these, let's summarise everything visually in **Step 6**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53904eef",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Visualise Animal Sentiments\n",
    "---\n",
    "\n",
    "We've now tagged each animal-related sentence with a **sentiment label** - positive, neutral, or negative - and a confidence score.\n",
    "\n",
    "Let's bring this data to life with some visualisations!\n",
    "\n",
    "We'll explore:\n",
    "\n",
    "- **How often each animal was mentioned**\n",
    "\n",
    "- **What the overall sentiment was for each one**\n",
    "\n",
    "- **Which animals got the most praise - or criticism**\n",
    "\n",
    "This helps us answer questions like:\n",
    "> _“Which animals were the stars of the zoo?”_  \n",
    "> _“Were there any surprises or mixed reviews?”_\n",
    "\n",
    "\n",
    "#### Most Positively Mentioned Animals (by Count)\n",
    "\n",
    "Let's start by looking at the **top 15 animals with the highest number of positive mentions**. These are the ones that visitors talked about most enthusiastically overall - not just because they were visible, but because they made an impression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d42c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare a summary table: count of sentiments per animal\n",
    "summary_table = (\n",
    "    animal_sentences_df.explode(\"species\")\n",
    "    .groupby([\"species\", \"Sentiment\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_values(by=\"positive\", ascending=False)\n",
    ")\n",
    "\n",
    "# Plot the top 15 animals by number of positive mentions\n",
    "top_animals = summary_table.head(15)\n",
    "\n",
    "top_animals[[\"negative\", \"neutral\", \"positive\"]].plot(\n",
    "    kind=\"barh\",\n",
    "    stacked=True,\n",
    "    figsize=(10, 8),\n",
    "    color=[\"#e74c3c\", \"#f1c40f\", \"#2ecc71\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Mentions\")\n",
    "plt.ylabel(\"Animal\")\n",
    "plt.title(\"Sentiment Breakdown for Top 15 Most Positively Mentioned Animals\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b30a0",
   "metadata": {},
   "source": [
    "#### Most Loved Animals (by Proportion)\n",
    "\n",
    "Sometimes, animals that aren't mentioned very often still generate **overwhelmingly positive** feedback when they are.\n",
    "\n",
    "In this chart, we look at the **top 10 animals with the highest proportion of positive mentions** - so we can see who really impressed their fans, even if they weren't the most famous residents.\n",
    "\n",
    "To keep things fair, we only include animals that had at least 5 total mentions. This avoids misleading results from animals that were only mentioned once - for example, a single glowing sentence might make an obscure animal seem like the most loved overall, even if no one else talked about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001098ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add total mentions\n",
    "summary_table[\"Total\"] = summary_table.sum(axis=1)\n",
    "\n",
    "# Filter for animals with at least 5 total mentions\n",
    "filtered = summary_table[summary_table[\"Total\"] >= 5].copy()\n",
    "\n",
    "# Calculate proportion of positive mentions\n",
    "filtered[\"Positive Proportion\"] = filtered[\"positive\"] / filtered[\"Total\"]\n",
    "\n",
    "# Sort by positive proportion\n",
    "most_positively_perceived = filtered.sort_values(\"Positive Proportion\", ascending=False).head(10)\n",
    "\n",
    "# Plot\n",
    "most_positively_perceived[[\"negative\", \"neutral\", \"positive\"]].plot(\n",
    "    kind=\"barh\",\n",
    "    stacked=True,\n",
    "    figsize=(10, 6),\n",
    "    color=[\"#e74c3c\", \"#f1c40f\", \"#2ecc71\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Mentions\")\n",
    "plt.ylabel(\"Animal\")\n",
    "plt.title(\"Top 10 Most Positively Perceived Animals (By Proportion)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053970ae",
   "metadata": {},
   "source": [
    "#### Most Criticised Animals (by Count)\n",
    "\n",
    "Now let's flip our attention to animals that received **the most negative mentions overall**.\n",
    "\n",
    "This helps answer questions like:\n",
    "\n",
    "> _“Which animals did visitors complain about most?”_  \n",
    "> _“Are there animals that consistently came up in negative reviews?”_\n",
    "\n",
    "This chart shows the **top 10 animals with the highest total number of negative mentions**, regardless of how often they were praised or mentioned neutrally.\n",
    "\n",
    "**Keep in mind:**\n",
    "Animals on this list might not be universally disliked - but they may have:\n",
    "\n",
    "- Been **hard to spot**\n",
    "\n",
    "- Had **high expectations** attached to them\n",
    "\n",
    "- Lived in **underwhelming enclosures**\n",
    "\n",
    "- Or had **other issues** that disappointed visitors\n",
    "\n",
    "\n",
    "Try writing the code **yourself** to generate this plot.\n",
    "\n",
    "<div style=\"\n",
    "  background: #e8f8f5;\n",
    "  border-left: 6px solid #1abc9c;\n",
    "  padding: 10px 16px;\n",
    "  border-radius: 6px;\n",
    "  margin-top: 12px;\n",
    "  margin-bottom: 12px;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "\">\n",
    "  <strong>Hint:</strong>\n",
    "  This task is very similar to the chart you created for <em>Most Positively Mentioned Animals (by Count)</em> - but now you'll want to <strong>sort by negative mentions</strong> instead.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786db00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2615ab3",
   "metadata": {},
   "source": [
    "<details style=\"\n",
    "  background: #fef9e7;\n",
    "  border-left: 6px solid #f4d03f;\n",
    "  padding: 10px;\n",
    "  border-radius: 8px;\n",
    "  margin-top: 10px;\n",
    "  margin-bottom: 12px;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "\">\n",
    "<summary style=\"font-weight: bold; cursor: pointer;\">Click to see the solution if you get stuck</summary>\n",
    "\n",
    "<div style=\"overflow-x: auto; padding-top: 10px;\">\n",
    "\n",
    "```python\n",
    "# Sort by number of negative mentions\n",
    "most_negative_by_count = summary_table.sort_values(\"negative\", ascending=False).head(10)\n",
    "\n",
    "# Plot\n",
    "most_negative_by_count[[\"negative\", \"neutral\", \"positive\"]].plot(\n",
    "    kind=\"barh\",\n",
    "    stacked=True,\n",
    "    figsize=(10, 6),\n",
    "    color=[\"#e74c3c\", \"#f1c40f\", \"#2ecc71\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Mentions\")\n",
    "plt.ylabel(\"Animal\")\n",
    "plt.title(\"Top 10 Most Criticised Animals (By Count)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f67ba",
   "metadata": {},
   "source": [
    "#### Most Criticised Animals (by Proportion)\n",
    "\n",
    "Instead of raw counts, let's now consider **which animals received a higher share of negative feedback relative to how often they were mentioned**.\n",
    "\n",
    "This helps surface animals that might not have been criticised the most overall - but when they were talked about, the tone wasn't great.\n",
    "\n",
    "*We'll again limit to animals with **at least 5 total mentions**.*\n",
    "\n",
    "\n",
    "Have a go at writing the code yourself!\n",
    "\n",
    "<div style=\"\n",
    "  background: #e8f8f5;\n",
    "  border-left: 6px solid #1abc9c;\n",
    "  padding: 10px 16px;\n",
    "  border-radius: 6px;\n",
    "  margin-top: 12px;\n",
    "  margin-bottom: 12px;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "\">\n",
    "  <strong>Hint:</strong> Think back to how we calculated the <em>positive proportion</em> earlier - this is almost identical.<br><br>\n",
    "  What needs to change is:\n",
    "  <ul style=\"margin-top: 4px; margin-bottom: 0;\">\n",
    "    <li>Which column you divide by total</li>\n",
    "    <li>Which column you sort by</li>\n",
    "    <li>And of course, the chart title and labels</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe743a0",
   "metadata": {},
   "source": [
    "<details style=\"\n",
    "  background: #fef9e7;\n",
    "  border-left: 6px solid #f4d03f;\n",
    "  padding: 10px;\n",
    "  border-radius: 8px;\n",
    "  margin-top: 10px;\n",
    "  margin-bottom: 12px;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "\">\n",
    "<summary style=\"font-weight: bold; cursor: pointer;\">Click to see the solution if you get stuck</summary>\n",
    "\n",
    "<div style=\"overflow-x: auto; padding-top: 10px;\">\n",
    "\n",
    "```python\n",
    "# Calculate proportion of negative mentions\n",
    "filtered[\"Negative Proportion\"] = filtered[\"negative\"] / filtered[\"Total\"]\n",
    "\n",
    "# Sort by negative proportion\n",
    "most_negatively_perceived = filtered.sort_values(\"Negative Proportion\", ascending=False).head(10)\n",
    "\n",
    "# Plot\n",
    "most_negatively_perceived[[\"negative\", \"neutral\", \"positive\"]].plot(\n",
    "    kind=\"barh\",\n",
    "    stacked=True,\n",
    "    figsize=(10, 6),\n",
    "    color=[\"#e74c3c\", \"#f1c40f\", \"#2ecc71\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Mentions\")\n",
    "plt.ylabel(\"Animal\")\n",
    "plt.title(\"Top 10 Most Criticised Animals (By Proportion)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "</div>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81ad09",
   "metadata": {},
   "source": [
    "#### Why Does This Matter?\n",
    "\n",
    "These sentiment insights aren't just fun - they're genuinely **valuable for zoo staff and management**.\n",
    "\n",
    "Knowing which animals people love most (and why) can help with:\n",
    "\n",
    "- **Marketing and social media:** Showcasing crowd favourites\n",
    "\n",
    "- **Signage and storytelling:** Adding extra info near popular enclosures\n",
    "\n",
    "- **Experience design:** Improving visibility or engagement around less-loved animals\n",
    "\n",
    "- **Education efforts:** Highlighting lesser-known species that people do enjoy\n",
    "\n",
    "And when an animal consistently draws negative mentions, it might be worth investigating:\n",
    "\n",
    "- Is the enclosure hard to access?\n",
    "\n",
    "- Is the animal usually inactive or hidden?\n",
    "\n",
    "- Are expectations mismatched with what visitors experience?\n",
    "\n",
    "*In short:* this kind of data helps the zoo turn visitor feedback into **actionable insights**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad0479",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Why Do People Feel That Way?\n",
    "---\n",
    "\n",
    "We've seen which animals were mentioned most positively or negatively - both by **total number of mentions** and by **proportion of feedback**.\n",
    "\n",
    "But so far, we've only looked at **how often** people said good or bad things.\n",
    "\n",
    "We haven't yet explored:\n",
    "\n",
    "> _Why do people like or dislike each animal?_  \n",
    "> _What exactly are they saying about them?_\n",
    "\n",
    "For animals with just a few mentions, we could simply read through their comments.\n",
    "\n",
    "But what if an animal was mentioned **dozens or even hundreds of times**?\n",
    "\n",
    "Manually reading through all those sentences, annotating what people liked or disliked, and trying to summarise it would take **a very long time** - especially if we’re doing this for multiple animals.\n",
    "\n",
    "We'd need to:\n",
    "\n",
    "- Read and understand each sentence\n",
    "\n",
    "- Categorise or tag each opinion\n",
    "\n",
    "- Look for common themes\n",
    "\n",
    "- Then write a summary ourselves\n",
    "\n",
    "That's **slow**, **repetitive**, and prone to **human error or bias**.\n",
    "\n",
    "This is exactly where **language models** shine. They can process large amounts of text and generate a summary that captures the **core themes and sentiments**, giving us a quick but meaningful overview of what people are saying.\n",
    "\n",
    "Let's see how that works in Step 8!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1136d5d",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Summarise Visitor Opinions with a Language Model\n",
    "---\n",
    "\n",
    "Let's now use a **Large Language Model (LLM)** to help us make sense of what visitors actually said about each animal.\n",
    "\n",
    "Instead of reading through every sentence ourselves, we'll ask the model to:\n",
    "\n",
    "- Understand the full set of comments for one animal\n",
    "\n",
    "- Identify common themes and sentiments\n",
    "\n",
    "- Generate a short, natural-language summary\n",
    "\n",
    "This gives us a **quick and readable overview** - especially useful when we have dozens (or hundreds!) of mentions.\n",
    "\n",
    "#### What's a Language Model (LLM)?\n",
    "\n",
    "A **Language Model** is a type of artificial intelligence (AI) trained to understand and generate human language.\n",
    "\n",
    "- It has read **huge amounts of text** - like books, articles, and online conversations.\n",
    "\n",
    "- It has learned the **patterns of language** - how ideas connect, what words tend to go together, and what kinds of answers people expect.\n",
    "\n",
    "- You can give it a **prompt** (like “Summarise these sentences”) - and it will try to respond like a human would.\n",
    "\n",
    "When a Language Model is trained on billions of words and has billions of parameters (internal weights), we call it a **Large Language Model**, or **LLM**.\n",
    "\n",
    "Famous examples include:\n",
    "\n",
    "- ChatGPT\n",
    "\n",
    "- Claude\n",
    "\n",
    "- Mistral\n",
    "\n",
    "- LLaMA\n",
    "\n",
    "- Gemini\n",
    "\n",
    "#### How Will We Use It?\n",
    "\n",
    "We'll ask the LLM to **read all the visitor comments** about a specific animal and **generate a short summary**.\n",
    "\n",
    "This will help us quickly uncover:\n",
    "\n",
    "- Common reasons why people liked it\n",
    "\n",
    "- Typical issues or disappointments\n",
    "\n",
    "- Patterns in the feedback (e.g., excitement, boredom, confusion)\n",
    "\n",
    "This is **especially helpful** when:\n",
    "\n",
    "- There are **dozens or hundreds** of mentions\n",
    "\n",
    "- You want to **save time** and avoid reading everything manually\n",
    "\n",
    "- You need to make **decisions** about animal care, signage, or marketing\n",
    "\n",
    "\n",
    "#### How Does It Actually Work?\n",
    "\n",
    "1. We collect all the sentences where an animal was mentioned.\n",
    "\n",
    "2. We create a **prompt** like:\n",
    "   > “Below is a collection of review sentences that mention the *animal_name*. Summarise what people said specifically about the *animal_name*. Include common opinions, emotional reactions, and noteworthy experiences. Do not mention other animals or general zoo facilities. Only refer to the 'animal'. Visitor comments: *reviews_text*.”\n",
    "\n",
    "3. We feed the prompt and sentences into the LLM.\n",
    "\n",
    "4. The LLM generates a natural-language summary - just like a human might write.\n",
    "\n",
    "The result is a quick and human-readable **snapshot** of public opinion.\n",
    "\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "Here's what it might look like when we summarise two animals:\n",
    "\n",
    "<div style=\"\n",
    "  background: #eaf2f8;\n",
    "  border-left: 6px solid #3498db;\n",
    "  padding: 14px 16px;\n",
    "  border-radius: 6px;\n",
    "  margin-top: 16px;\n",
    "  margin-bottom: 14px;\n",
    "  max-width: 100%;\n",
    "  overflow-wrap: break-word;\n",
    "  box-sizing: border-box;\n",
    "\">\n",
    "  <p style=\"margin: 0; font-size: 14px;\"><strong>🐧 penguin</strong><br>\n",
    "  <em>→ Visitors frequently mentioned how playful and active the penguins were, especially when swimming. Some also noted they were fun to watch with kids. A few comments mentioned difficulty seeing them during warmer weather.</em></p>\n",
    "\n",
    "  <br>\n",
    "\n",
    "  <p style=\"margin: 0; font-size: 14px;\"><strong>🦒 giraffe</strong><br>\n",
    "  <em>→ People often described the giraffes as majestic and gentle. Many reviews praised how close visitors could get to them, though some noted long queues near the enclosure.</em></p>\n",
    "</div>\n",
    "\n",
    "\n",
    "In order to do this, we will start by **creating a file that collects sentences from the reviews** that talk about a given animal. We will then **copy the content of this file and feed it into ChatGPT together with our prompt**.\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 5px solid #ffc107;\n",
    "  background: #fff8e1;\n",
    "  padding: 0.75em 1em;\n",
    "  margin-top: 1em;\n",
    "  max-width: 100%;\n",
    "  box-sizing: border-box;\n",
    "  overflow-wrap: break-word;\n",
    "  overflow-x: auto;\n",
    "  border-radius: 6px;\n",
    "\">\n",
    "  <strong>Tip:</strong>\n",
    "  <ul style=\"margin-top: 4px; margin-bottom: 0;\">\n",
    "    <li>Use `ctrl + a` to select all content in a file</li>\n",
    "    <li>Use `ctrl + c` to copy the selected content</li>\n",
    "    <li>Use `ctrl + v` to paste the copied content</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set animals to summarise (make sure that the names match the ones in our dictionary from Step 4!)\n",
    "ANIMALS_TO_SUMMARISE = [\"Giant panda\"]  # change or extend this list as needed\n",
    "MAX_CHAR_PER_PROMPT = 10000  # safeguard for long inputs\n",
    "\n",
    "# Use this as your sentence-level DataFrame\n",
    "sentence_df = animal_sentences_df  # or rename earlier if preferred\n",
    "\n",
    "# Dictionary to store summaries\n",
    "animal_summaries = {}\n",
    "\n",
    "for animal in ANIMALS_TO_SUMMARISE:\n",
    "    # Step 1: Mention filter with robust matching\n",
    "    animal_sentences = sentence_df[\n",
    "        sentence_df[\"species\"].apply(\n",
    "            lambda x: isinstance(x, (list, tuple)) and any(animal.lower() == a.strip().lower() for a in x if isinstance(a, str))\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Step 2: Keep only opinionated sentences\n",
    "    animal_sentences = animal_sentences[\n",
    "        animal_sentences[\"Sentiment\"].isin([\"positive\", \"negative\"])\n",
    "    ]\n",
    "\n",
    "    if animal_sentences.empty:\n",
    "        animal_summaries[animal] = \"(No comments found for this animal.)\"\n",
    "        continue\n",
    "\n",
    "    # Combine sentences into one string (truncated if too long)\n",
    "    text_block = \"\\n\".join(animal_sentences[\"review_sentence\"].tolist())[:MAX_CHAR_PER_PROMPT]\n",
    "\n",
    "    text_file = open(f\"{animal.replace(' ', '_')}_comments.txt\", \"w\")\n",
    "    text_file.write(text_block)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b8b98",
   "metadata": {},
   "source": [
    "The output should be a `.txt` file with a collection of sentences referring to the animal we chose. Open it and see if the output is correct. If no file has been created, go back to the code block above and check that the name you wrote actually corresponds to one of the animals in the zoo, as shown in the dictionary in **Step 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95339f1",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus Task: Experiment, Reflect, and Recommend\n",
    "---\n",
    "\n",
    "Now that you've seen how a **Large Language Model (LLM)** can summarise visitor opinions, it's your turn to explore further!\n",
    "\n",
    "#### Play Around with the Prompt\n",
    "\n",
    "Can you get different types of summaries?\n",
    "\n",
    "- Try rewording the `prompt`:\n",
    "  - “Write a fun, informal review of what people think about the *animal*.”\n",
    "  - “List three common complaints people have about the *animal*'s enclosure.”\n",
    "\n",
    "- Make it more detailed, playful, or emotional - and see how the tone changes!\n",
    "\n",
    "#### Try Different Animals\n",
    "\n",
    "Change the `ANIMALS_TO_SUMMARISE` list to explore other animals.\n",
    "\n",
    "- Which animals have the most enthusiastic visitors?\n",
    "\n",
    "- Which get mixed or negative reactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ca437",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Reflection: Zoo Designers Wanted!\n",
    "---\n",
    "\n",
    "Imagine you work at the zoo and your job is to improve the visitor experience.\n",
    "\n",
    "Based on what you've read or summarised:\n",
    "\n",
    "- What patterns did you notice?\n",
    "\n",
    "- Which animals were most loved? Why?\n",
    "\n",
    "- What animals do people want the most Edinburgh adds to their zoo?\n",
    "\n",
    "- Were there any recurring issues people had?\n",
    "\n",
    "Write down 1-2 ideas you would suggest to the zoo:\n",
    "\n",
    "- It could be an improvement for one animal's enclosure\n",
    "\n",
    "- Or a new idea for signage, events, or volunteer guides\n",
    "\n",
    "Be creative - you're using data to make real-world decisions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sutton_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
