{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a98991d",
   "metadata": {},
   "source": [
    "# Evaluate the NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6901a8e",
   "metadata": {},
   "source": [
    "In preparation for this workshop, we have annotated a subset of 200 of those 2000 reviews, and trained a NER model with it. Training the model requires access to \"GPU\", graphical processing units ([explain what that is better]), which is a piece of hardware that is very useful for playing big videogames, and for running models. Unfortunately, this machine does not have access to a GPU, and training the model anyways takes too long! (you won't want to spend 10 minutes looking at the screen). So we have pretrained this model for you.\n",
    "\n",
    "As explained, we have divided it into a \"train\", \"validation\", and \"test\" dataset. We have used the \"train\" and \"validation\" datasets to train and get the best version of the model, but the model hasn't seen the \"test\" section of the data yet. We will load the model, and test it, to see what is its performance, and show a few examples of cases where it fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb82d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "# TODO - This is incorrect. Load the actual test set, already split into sentences\n",
    "import json\n",
    "test_annotations_path = \"animals_100_annotated.json\"\n",
    "with open(test_annotations_path, \"r\") as f:\n",
    "    test_ann = json.load(f)\n",
    "test_ann = test_ann[:30]\n",
    "\n",
    "# === Separate into sentences\n",
    "import spacy\n",
    "\n",
    "def extract_sentence_level_annotations(ann):\n",
    "    \"\"\"Split reviews into sentences and extract entity span offsets for each sentence.\"\"\"\n",
    "\n",
    "    # Load model to split sentences\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    annotated_data = []\n",
    "    for item in ann:\n",
    "        \n",
    "        # Separate into sentences using Spacy\n",
    "        review_text = item[\"data\"][\"Text\"]\n",
    "        doc = nlp(review_text)\n",
    "        for sent in doc.sents:\n",
    "            sent_spans = []\n",
    "    \n",
    "            # Get the annotations from that sentence\n",
    "            for annotation in item.get(\"annotations\", []):\n",
    "                for res in annotation.get(\"result\", []):\n",
    "                    v = res.get(\"value\", {})\n",
    "    \n",
    "                    # Add to sent_spans if the annotation is from this sentence\n",
    "                    if sent.start_char <= v['start'] and v['end'] < sent.end_char:\n",
    "                        sent_spans.append((v['start']-sent.start_char, v['end']-sent.start_char))\n",
    "    \n",
    "            # Add to annotated data\n",
    "            annotated_data.append({\n",
    "                \"text\": sent.text.strip(),\n",
    "                \"sent_spans\": sent_spans\n",
    "            })\n",
    "\n",
    "    # Return annotated data\n",
    "    return annotated_data\n",
    "    \n",
    "annotated_data = extract_sentence_level_annotations(test_ann)\n",
    "\n",
    "# Show an example of a sentence with an annotation.\n",
    "# As most sentences don't have an annotation, we must filter out for one that has:\n",
    "for example in annotated_data:\n",
    "    # Check if the sentence has an annotated animal:\n",
    "    if example[\"sent_spans\"] != []:\n",
    "        sent = example[\"text\"]\n",
    "        spans = example[\"sent_spans\"]\n",
    "        print(\"Sentence:\\t\", sent)\n",
    "        print(\"Spans:\\t\", spans)\n",
    "        print(\"Animals:\\t\", \", \".join(sent[start:end] for start, end in spans))\n",
    "\n",
    "        # Break the loop to only see one example\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5856df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE MODEL AND THE TOKENIZER\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "LOAD_MODEL_PATH = \"animal-ner-model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOAD_MODEL_PATH)\n",
    "model = AutoModelForTokenClassification.from_pretrained(LOAD_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3dbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE DATA FOR EVALUATING\n",
    "from datasets import Dataset\n",
    "\n",
    "# === Define label schema\n",
    "label_list = [\"O\", \"B-ANIMAL\", \"I-ANIMAL\"]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# === Convert into Dataset for training\n",
    "dataset = Dataset.from_list(annotated_data)\n",
    "\n",
    "# === Tokenize + align labels\n",
    "def align_tokens_with_labels(example):\n",
    "    text = example[\"text\"]\n",
    "    spans = example[\"sent_spans\"]\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for offset in encoding[\"offset_mapping\"]:\n",
    "        start, end = offset\n",
    "        if start == end:\n",
    "            labels.append(-100)\n",
    "            continue\n",
    "\n",
    "        label = \"O\"\n",
    "        for span_start, span_end in spans:\n",
    "            if start == span_start:\n",
    "                label = \"B-ANIMAL\"\n",
    "                break\n",
    "            elif span_start < start < span_end:\n",
    "                label = \"I-ANIMAL\"\n",
    "                break\n",
    "        labels.append(label_to_id[label])\n",
    "\n",
    "    encoding.pop(\"offset_mapping\")\n",
    "    encoding[\"labels\"] = labels\n",
    "    encoding[\"text\"] = text\n",
    "    return encoding\n",
    "\n",
    "tokenized_dataset = dataset.map(align_tokens_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one sample from the tokenized dataset\n",
    "def inspect_dataset(tokenized_dataset, i=3) -> None:\n",
    "    '''\n",
    "    Change i to get other examples\n",
    "    '''\n",
    "    sample = tokenized_dataset[i]\n",
    "    \n",
    "    print(\"Original Text:\")\n",
    "    print(dataset[i][\"text\"])\n",
    "    print(\"\\nTokenized Tokens:\")\n",
    "    print(tokenizer.convert_ids_to_tokens(sample[\"input_ids\"]))\n",
    "    print(\"\\nLabels:\")\n",
    "    print([id for id in sample[\"labels\"]])\n",
    "    \n",
    "    # Decode label ids back to strings (optional)\n",
    "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "    decoded_labels = [id_to_label.get(id, \"PAD\") if id != -100 else \"IGN\" for id in sample[\"labels\"]]\n",
    "    print(\"\\nDecoded Labels:\")\n",
    "    print(decoded_labels)\n",
    "\n",
    "inspect_dataset(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE THE DATASET\n",
    "# TODO - I don't know how to do that without the trainer. Ask ChatGPT\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Process reviews in batches\n",
    "for i in tqdm(range(0, len(tokenized_dataset), batch_size), desc=\"Finding animals in text\"):\n",
    "    batch_sents = tokenized_dataset[i:i+batch_size]\n",
    "\n",
    "    # Tokenize all sentences in the batch with padding, truncation, and offset mapping\n",
    "    encodings = tokenizer(\n",
    "        batch_sents,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    # Get input tensors\n",
    "    input_ids = encodings[\"input_ids\"]\n",
    "    attention_mask = encodings[\"attention_mask\"]\n",
    "    offset_mappings = encodings[\"offset_mapping\"]\n",
    "\n",
    "    # Run model inference without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get predicted label indices\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    for sent_idx, sentence in enumerate(sentences):\n",
    "        # Get model output for this sentence\n",
    "        preds = predictions[sent_idx]           # predicted labels (as IDs) for each token\n",
    "        input_ids_sent = input_ids[sent_idx]    # the token IDs for the sentence (what was passed into the model).\n",
    "        offsets = offset_mappings[sent_idx]     # for each token, this tells use where the original string of the token came from"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
